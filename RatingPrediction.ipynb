{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restaurant Recommendation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restaurant recommendation\n",
    "\n",
    "The goal of this task is to recommend restaurants to users based on the rating data in the Yelp dataset. For this, we try to predict the rating a user will give to a restaurant they have not yet rated based on a latent factor model.\n",
    "\n",
    "Specifically, the objective function (loss) we wanted to optimize is:\n",
    "$$\n",
    "\\mathcal{L} = \\min_{P, Q} \\sum_{(i, x) \\in W} (M_{ix} - \\mathbf{q}_i^T\\mathbf{p}_x)^2 + \\lambda\\sum_x{\\left\\lVert \\mathbf{p}_x  \\right\\rVert}^2 + \\lambda\\sum_i {\\left\\lVert\\mathbf{q}_i  \\right\\rVert}^2\n",
    "$$\n",
    "\n",
    "where $W$ is the set of $(i, x)$ pairs for which the rating $M_{ix}$ given by user $i$ to restaurant $x$ is known. Here we have also introduced two regularization terms to help us with overfitting where $\\lambda$ is hyper-parameter that control the strength of the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.load(\"ratings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101968,   1880,      1],\n",
       "       [101968,    284,      5],\n",
       "       [101968,   1378,      2],\n",
       "       ...,\n",
       "       [ 72452,   2100,      4],\n",
       "       [ 72452,   2050,      5],\n",
       "       [ 74861,   3979,      5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have triplets of (user, restaurant, rating).\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the data into a matrix of dimension [N, D], where N is the number of users and D is the number of restaurants in the dataset. We store the data as a sparse matrix to avoid out-of-memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<337867x5899 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 929606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = np.max(ratings[:,0] + 1)\n",
    "n_restaurants = np.max(ratings[:,1] + 1)\n",
    "M = sp.coo_matrix((ratings[:,2], (ratings[:,0], ratings[:,1])), shape=(n_users, n_restaurants)).tocsr()\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the <a href=\"https://en.wikipedia.org/wiki/Cold_start_(computing)\"> cold start problem</a>, in the preprocessing step, we recursively remove all users and restaurants with 10 or less ratings.\n",
    "\n",
    "Then, we randomly select 200 data points for the validation and test sets, respectively.\n",
    "\n",
    "After this, we subtract the mean rating for each users to account for this global effect.\n",
    "\n",
    "**Note**: Some entries might become zero in this process -- but these entries are different than the 'unknown' zeros in the matrix. We store the indices for which we the rating data available in a separate variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cold_start_preprocessing(matrix, min_entries):\n",
    "    \"\"\"\n",
    "    Recursively removes rows and columns from the input matrix which have less than min_entries nonzero entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix      : sp.spmatrix, shape [N, D]\n",
    "                  The input matrix to be preprocessed.\n",
    "    min_entries : int\n",
    "                  Minimum number of nonzero elements per row and column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix      : sp.spmatrix, shape [N', D']\n",
    "                  The pre-processed matrix, where N' <= N and D' <= D\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Shape before: {}\".format(matrix.shape))\n",
    "    \n",
    "    shape = (-1, -1)\n",
    "    while matrix.shape != shape:\n",
    "        shape = matrix.shape\n",
    "        nnz = matrix>0\n",
    "        row_ixs = nnz.sum(1).A1 > min_entries\n",
    "        matrix = matrix[row_ixs]\n",
    "        nnz = matrix>0\n",
    "        col_ixs = nnz.sum(0).A1 > min_entries\n",
    "        matrix = matrix[:,col_ixs]\n",
    "    print(\"Shape after: {}\".format(matrix.shape))\n",
    "    nnz = matrix>0\n",
    "    assert (nnz.sum(0).A1 > min_entries).all()\n",
    "    assert (nnz.sum(1).A1 > min_entries).all()\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data by substracting the mean user rating from the sparse rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_user_mean(matrix):\n",
    "    \"\"\"\n",
    "    Subtract the mean rating per user from the non-zero elements in the input matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             Input sparse matrix.\n",
    "    Returns\n",
    "    -------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The modified input matrix.\n",
    "    \n",
    "    user_means : np.array, shape [N, 1]\n",
    "                 The mean rating per user that can be used to recover the absolute ratings from the mean-shifted ones.\n",
    "\n",
    "    \"\"\"\n",
    "      \n",
    "    nnz_mask = (matrix > 0)\n",
    "    user_means = matrix.sum(1) / nnz_mask.sum(1)\n",
    "    subtract_mask = sp.csr_matrix(user_means).multiply(nnz_mask)\n",
    "    matrix = matrix - subtract_mask\n",
    "    \n",
    "    assert np.all(np.isclose(matrix.mean(1), 0))\n",
    "    return matrix, user_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a train, validation and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(matrix, n_validation, n_test):\n",
    "    \"\"\"\n",
    "    Extract validation and test entries from the input matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix          : sp.spmatrix, shape [N, D]\n",
    "                      The input data matrix.\n",
    "    n_validation    : int\n",
    "                      The number of validation entries to extract.\n",
    "    n_test          : int\n",
    "                      The number of test entries to extract.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matrix_split    : sp.spmatrix, shape [N, D]\n",
    "                      A copy of the input matrix in which the validation and test entries have been set to zero.\n",
    "    \n",
    "    val_idx         : tuple, shape [2, n_validation]\n",
    "                      The indices of the validation entries.\n",
    "    \n",
    "    test_idx        : tuple, shape [2, n_test]\n",
    "                      The indices of the test entries.\n",
    "    \n",
    "    val_values      : np.array, shape [n_validation, ]\n",
    "                      The values of the input matrix at the validation indices.\n",
    "                      \n",
    "    test_values     : np.array, shape [n_test, ]\n",
    "                      The values of the input matrix at the test indices.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix_cp = matrix.copy()\n",
    "    non_zero_idx = np.argwhere(matrix_cp)\n",
    "    ixs = np.random.permutation(non_zero_idx)\n",
    "    val_idx = tuple(ixs[:n_validation].T)\n",
    "    test_idx = tuple(ixs[n_validation:n_validation + n_test].T)\n",
    "    \n",
    "    val_values = matrix_cp[val_idx].A1\n",
    "    test_values = matrix_cp[test_idx].A1\n",
    "    \n",
    "    matrix_cp[val_idx] = matrix_cp[test_idx] = 0\n",
    "    matrix_cp.eliminate_zeros()\n",
    "\n",
    "    return matrix_cp, val_idx, test_idx, val_values, test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (337867, 5899)\n",
      "Shape after: (3529, 2072)\n"
     ]
    }
   ],
   "source": [
    "M = cold_start_preprocessing(M, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation = 200\n",
    "n_test = 200\n",
    "# Split data\n",
    "M_train, val_idx, test_idx, val_values, test_values = split_data(M, n_validation, n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove user means.\n",
    "nonzero_indices = np.argwhere(M_train)\n",
    "M_shifted, user_means = shift_user_mean(M_train)\n",
    "# Apply the same shift to the validation and test data.\n",
    "val_values_shifted = val_values - user_means[np.array(val_idx).T[:,0]].A1\n",
    "test_values_shifted = test_values - user_means[np.array(test_idx).T[:,0]].A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(values, ixs, Q, P, reg_lambda):\n",
    "    \"\"\"\n",
    "    Compute the loss of the latent factor model (at indices ixs).\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : np.array, shape ~[n_ixs,]\n",
    "        The array with the ground-truth values.\n",
    "    ixs : tuple, shape [2, n_ixs]\n",
    "        The indices at which we want to evaluate the loss (usually the nonzero indices of the unshifted data matrix).\n",
    "    Q : np.array, shape [N, k]\n",
    "        The matrix Q of a latent factor model.\n",
    "    P : np.array, shape [k, D]\n",
    "        The matrix P of a latent factor model.\n",
    "    reg_lambda : float\n",
    "        The regularization strength\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "           The loss of the latent factor model.\n",
    "\n",
    "    \"\"\"\n",
    "    mean_sse_loss = np.sum((values - Q.dot(P)[ixs])**2)\n",
    "    regularization_loss =  reg_lambda * (np.sum(np.linalg.norm(P, axis=0)**2) + np.sum(np.linalg.norm(Q, axis=1) ** 2))\n",
    "    \n",
    "    return mean_sse_loss + regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating optimization\n",
    "\n",
    "In the first step, we will approach the problem via alternating optimization, as learned in the lecture. That is, during each iteration you first update $Q$ while having $P$ fixed and then vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that initializes the latent factors $Q$ and $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Q_P(matrix, k, init='random'):\n",
    "    \"\"\"\n",
    "    Initialize the matrices Q and P for a latent factor model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : sp.spmatrix, shape [N, D]\n",
    "             The matrix to be factorized.\n",
    "    k      : int\n",
    "             The number of latent dimensions.\n",
    "    init   : str in ['svd', 'random'], default: 'random'\n",
    "             The initialization strategy. 'svd' means that we use SVD to initialize P and Q, 'random' means we initialize\n",
    "             the entries in P and Q randomly in the interval [0, 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : np.array, shape [N, k]\n",
    "        The initialized matrix Q of a latent factor model.\n",
    "\n",
    "    P : np.array, shape [k, D]\n",
    "        The initialized matrix P of a latent factor model.\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    if init == 'svd':\n",
    "        U, s, V = svds(matrix, k=k)\n",
    "        S = np.diag(s)\n",
    "        Q = U.dot(S)\n",
    "        P = V\n",
    "    elif init == 'random':\n",
    "        Q = np.random.random((matrix.shape[0], k))\n",
    "        P = np.random.random((k, matrix.shape[1]))\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    assert Q.shape == (matrix.shape[0], k)\n",
    "    assert P.shape == (k, matrix.shape[1])\n",
    "    return Q, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating optimization approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_factor_alternating_optimization(M, non_zero_idx, k, val_idx, val_values,\n",
    "                                           reg_lambda, max_steps=100, init='random',\n",
    "                                           log_every=1, patience=5, eval_every=1):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using alternating optimization. Training is done via patience,\n",
    "    i.e. we stop training after we observe no improvement on the validation loss for a certain\n",
    "    amount of training steps. We then return the best values for Q and P oberved during training.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M                 : sp.spmatrix, shape [N, D]\n",
    "                        The input matrix to be factorized.\n",
    "                      \n",
    "    non_zero_idx      : np.array, shape [nnz, 2]\n",
    "                        The indices of the non-zero entries of the un-shifted matrix to be factorized. \n",
    "                        nnz refers to the number of non-zero entries. Note that this may be different\n",
    "                        from the number of non-zero entries in the input matrix M, e.g. in the case\n",
    "                        that all ratings by a user have the same value.\n",
    "    \n",
    "    k                 : int\n",
    "                        The latent factor dimension.\n",
    "    \n",
    "    val_idx           : tuple, shape [2, n_validation]\n",
    "                        Tuple of the validation set indices.\n",
    "                        n_validation refers to the size of the validation set.\n",
    "                      \n",
    "    val_values        : np.array, shape [n_validation, ]\n",
    "                        The values in the validation set.\n",
    "                      \n",
    "    reg_lambda        : float\n",
    "                        The regularization strength.\n",
    "                      \n",
    "    max_steps         : int, optional, default: 100\n",
    "                        Maximum number of training steps. Note that we will stop early if we observe\n",
    "                        no improvement on the validation error for a specified number of steps\n",
    "                        (see \"patience\" for details).\n",
    "                      \n",
    "    init              : str in ['random', 'svd'], default 'random'\n",
    "                        The initialization strategy for P and Q. See function initialize_Q_P for details.\n",
    "    \n",
    "    log_every         : int, optional, default: 1\n",
    "                        Log the training status every X iterations.\n",
    "                    \n",
    "    patience          : int, optional, default: 5\n",
    "                        Stop training after we observe no improvement of the validation loss for X evaluation\n",
    "                        iterations (see eval_every for details). After we stop training, we restore the best \n",
    "                        observed values for Q and P (based on the validation loss) and return them.\n",
    "                      \n",
    "    eval_every        : int, optional, default: 1\n",
    "                        Evaluate the training and validation loss every X steps. If we observe no improvement\n",
    "                        of the validation error, we decrease our patience by 1, else we reset it to *patience*.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_Q            : np.array, shape [N, k]\n",
    "                        Best value for Q (based on validation loss) observed during training\n",
    "                      \n",
    "    best_P            : np.array, shape [k, D]\n",
    "                        Best value for P (based on validation loss) observed during training\n",
    "                      \n",
    "    validation_losses : list of floats\n",
    "                        Validation loss for every evaluation iteration, can be used for plotting the validation\n",
    "                        loss over time.\n",
    "                        \n",
    "    train_losses      : list of floats\n",
    "                        Training loss for every evaluation iteration, can be used for plotting the training\n",
    "                        loss over time.                     \n",
    "    \n",
    "    converged_after   : int\n",
    "                        it - patience*eval_every, where it is the iteration in which patience hits 0,\n",
    "                        or -1 if we hit max_steps before converging. \n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    nnz_mask = sp.coo_matrix((np.ones(len(non_zero_idx)),\n",
    "                              (non_zero_idx[:,0],non_zero_idx[:,1])),\n",
    "                             shape=M.shape, dtype=\"uint8\").tocsr()\n",
    "    nnz_mask_col = nnz_mask.tocsc()\n",
    "    \n",
    "    cols = nnz_mask.T.tolil().rows\n",
    "    rows = nnz_mask.tolil().rows\n",
    "\n",
    "    reg = Ridge(alpha=reg_lambda, fit_intercept=False)\n",
    "    \n",
    "    Q,P = initialize_Q_P(M, k, init)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    best_val_loss = best_Q = best_P = converged_after = -1\n",
    "    train_idx = tuple(non_zero_idx.T)\n",
    "    \n",
    "    bef = -1\n",
    "    times = []\n",
    "    for it in range(max_steps):\n",
    "        if bef != -1:\n",
    "            times.append(time.time()-bef)\n",
    "        bef = time.time()\n",
    "        \n",
    "        if it % eval_every == 0:\n",
    "            train_loss = loss(M[train_idx].A1, train_idx, Q, P, reg_lambda)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            val_loss = loss(val_values, val_idx, Q, P, reg_lambda)\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            if best_val_loss < 0 or val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_Q = Q\n",
    "                best_P = P\n",
    "                current_patience = patience\n",
    "            else:\n",
    "                current_patience -= 1\n",
    "\n",
    "            if current_patience == 0:\n",
    "                converged_after = it - patience*eval_every\n",
    "                break        \n",
    "            \n",
    "        print(\"Iteration {}, training loss: {:.3f}, validation loss: {:.3f}\".format(it, train_loss, val_loss))\n",
    "        \n",
    "        # fix Q and update P\n",
    "        for rating_idx in range(M.shape[1]):\n",
    "            nnz_idx = cols[rating_idx]\n",
    "            res = reg.fit(Q[nnz_idx], np.squeeze(M[nnz_idx, rating_idx].toarray()))\n",
    "            P[:, rating_idx] = res.coef_\n",
    "\n",
    "        for user_idx in range(M.shape[0]):\n",
    "            nnz_idx = rows[user_idx]\n",
    "            res = reg.fit(P[:, nnz_idx].T, np.squeeze(M[user_idx, nnz_idx].toarray()))\n",
    "            Q[user_idx, :] = res.coef_\n",
    "    print(\"Converged after {} iterations, on average {:.3f}s per iteration\".format(converged_after, np.mean(times)))\n",
    "    return best_Q, best_P, validation_losses, train_losses, converged_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the latent factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, training loss: 96802452.587, validation loss: 128431.220\n",
      "Iteration 1, training loss: 2231.704, validation loss: 2974.661\n",
      "Iteration 2, training loss: 512.997, validation loss: 2516.252\n",
      "Iteration 3, training loss: 198.280, validation loss: 1993.096\n",
      "Iteration 4, training loss: 98.273, validation loss: 1587.819\n",
      "Iteration 5, training loss: 56.413, validation loss: 1521.075\n",
      "Iteration 6, training loss: 35.806, validation loss: 1339.632\n",
      "Iteration 7, training loss: 24.708, validation loss: 1261.062\n",
      "Iteration 8, training loss: 18.351, validation loss: 1064.552\n",
      "Iteration 9, training loss: 14.587, validation loss: 1085.118\n",
      "Iteration 10, training loss: 12.245, validation loss: 1093.827\n",
      "Iteration 11, training loss: 10.743, validation loss: 1102.885\n",
      "Iteration 12, training loss: 9.750, validation loss: 1092.077\n",
      "Iteration 13, training loss: 9.074, validation loss: 1074.674\n",
      "Iteration 14, training loss: 8.607, validation loss: 1053.411\n",
      "Iteration 15, training loss: 8.276, validation loss: 1036.252\n",
      "Iteration 16, training loss: 8.036, validation loss: 1010.286\n",
      "Iteration 17, training loss: 7.858, validation loss: 992.360\n",
      "Iteration 18, training loss: 7.723, validation loss: 970.071\n",
      "Iteration 19, training loss: 7.619, validation loss: 956.299\n",
      "Iteration 20, training loss: 7.538, validation loss: 942.809\n",
      "Iteration 21, training loss: 7.473, validation loss: 931.632\n",
      "Iteration 22, training loss: 7.419, validation loss: 921.513\n",
      "Iteration 23, training loss: 7.374, validation loss: 912.022\n",
      "Iteration 24, training loss: 7.335, validation loss: 903.148\n",
      "Iteration 25, training loss: 7.300, validation loss: 894.773\n",
      "Iteration 26, training loss: 7.270, validation loss: 887.209\n",
      "Iteration 27, training loss: 7.243, validation loss: 880.403\n",
      "Iteration 28, training loss: 7.217, validation loss: 873.669\n",
      "Iteration 29, training loss: 7.194, validation loss: 867.323\n",
      "Iteration 30, training loss: 7.173, validation loss: 861.676\n",
      "Iteration 31, training loss: 7.152, validation loss: 856.511\n",
      "Iteration 32, training loss: 7.133, validation loss: 851.677\n",
      "Iteration 33, training loss: 7.114, validation loss: 847.063\n",
      "Iteration 34, training loss: 7.096, validation loss: 842.623\n",
      "Iteration 35, training loss: 7.079, validation loss: 838.341\n",
      "Iteration 36, training loss: 7.063, validation loss: 834.217\n",
      "Iteration 37, training loss: 7.047, validation loss: 830.250\n",
      "Iteration 38, training loss: 7.031, validation loss: 826.444\n",
      "Iteration 39, training loss: 7.016, validation loss: 822.809\n",
      "Iteration 40, training loss: 7.002, validation loss: 819.367\n",
      "Iteration 41, training loss: 6.987, validation loss: 816.157\n",
      "Iteration 42, training loss: 6.973, validation loss: 813.268\n",
      "Iteration 43, training loss: 6.959, validation loss: 810.698\n",
      "Iteration 44, training loss: 6.946, validation loss: 807.834\n",
      "Iteration 45, training loss: 6.933, validation loss: 805.137\n",
      "Iteration 46, training loss: 6.920, validation loss: 802.538\n",
      "Iteration 47, training loss: 6.908, validation loss: 800.023\n",
      "Iteration 48, training loss: 6.896, validation loss: 797.597\n",
      "Iteration 49, training loss: 6.884, validation loss: 795.272\n",
      "Iteration 50, training loss: 6.872, validation loss: 793.052\n",
      "Iteration 51, training loss: 6.860, validation loss: 790.934\n",
      "Iteration 52, training loss: 6.849, validation loss: 788.910\n",
      "Iteration 53, training loss: 6.837, validation loss: 786.968\n",
      "Iteration 54, training loss: 6.826, validation loss: 785.101\n",
      "Iteration 55, training loss: 6.816, validation loss: 783.300\n",
      "Iteration 56, training loss: 6.805, validation loss: 781.558\n",
      "Iteration 57, training loss: 6.794, validation loss: 779.868\n",
      "Iteration 58, training loss: 6.784, validation loss: 778.227\n",
      "Iteration 59, training loss: 6.774, validation loss: 776.630\n",
      "Iteration 60, training loss: 6.764, validation loss: 775.073\n",
      "Iteration 61, training loss: 6.754, validation loss: 773.554\n",
      "Iteration 62, training loss: 6.744, validation loss: 772.070\n",
      "Iteration 63, training loss: 6.734, validation loss: 770.620\n",
      "Iteration 64, training loss: 6.725, validation loss: 769.202\n",
      "Iteration 65, training loss: 6.716, validation loss: 767.815\n",
      "Iteration 66, training loss: 6.706, validation loss: 766.457\n",
      "Iteration 67, training loss: 6.697, validation loss: 765.128\n",
      "Iteration 68, training loss: 6.688, validation loss: 763.826\n",
      "Iteration 69, training loss: 6.679, validation loss: 762.550\n",
      "Iteration 70, training loss: 6.671, validation loss: 761.300\n",
      "Iteration 71, training loss: 6.662, validation loss: 760.075\n",
      "Iteration 72, training loss: 6.653, validation loss: 758.874\n",
      "Iteration 73, training loss: 6.645, validation loss: 757.695\n",
      "Iteration 74, training loss: 6.637, validation loss: 756.538\n",
      "Iteration 75, training loss: 6.628, validation loss: 755.403\n",
      "Iteration 76, training loss: 6.620, validation loss: 754.287\n",
      "Iteration 77, training loss: 6.612, validation loss: 753.191\n",
      "Iteration 78, training loss: 6.604, validation loss: 752.114\n",
      "Iteration 79, training loss: 6.596, validation loss: 751.054\n",
      "Iteration 80, training loss: 6.589, validation loss: 750.011\n",
      "Iteration 81, training loss: 6.581, validation loss: 748.985\n",
      "Iteration 82, training loss: 6.573, validation loss: 747.974\n",
      "Iteration 83, training loss: 6.566, validation loss: 746.978\n",
      "Iteration 84, training loss: 6.558, validation loss: 745.997\n",
      "Iteration 85, training loss: 6.551, validation loss: 745.029\n",
      "Iteration 86, training loss: 6.544, validation loss: 744.074\n",
      "Iteration 87, training loss: 6.536, validation loss: 743.133\n",
      "Iteration 88, training loss: 6.529, validation loss: 742.203\n",
      "Iteration 89, training loss: 6.522, validation loss: 741.285\n",
      "Iteration 90, training loss: 6.515, validation loss: 740.379\n",
      "Iteration 91, training loss: 6.508, validation loss: 739.484\n",
      "Iteration 92, training loss: 6.501, validation loss: 738.599\n",
      "Iteration 93, training loss: 6.494, validation loss: 737.725\n",
      "Iteration 94, training loss: 6.488, validation loss: 736.861\n",
      "Iteration 95, training loss: 6.481, validation loss: 736.008\n",
      "Iteration 96, training loss: 6.474, validation loss: 735.164\n",
      "Iteration 97, training loss: 6.468, validation loss: 734.331\n",
      "Iteration 98, training loss: 6.461, validation loss: 733.507\n",
      "Iteration 99, training loss: 6.455, validation loss: 732.693\n",
      "Converged after -1 iterations, on average 11.142s per iteration\n"
     ]
    }
   ],
   "source": [
    "Q, P, val_loss, train_loss, converged = latent_factor_alternating_optimization(M_shifted, nonzero_indices, \n",
    "                                                                               k=100, val_idx=val_idx,\n",
    "                                                                               val_values=val_values_shifted, \n",
    "                                                                               reg_lambda=1e-4, init='random',\n",
    "                                                                               max_steps=100, patience=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the validation and training losses over for each iteration (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[10, 5])\n",
    "fig.suptitle(\"Alternating optimization, k=100\")\n",
    "\n",
    "ax[0].plot(train_loss[1::])\n",
    "ax[0].set_title('Training loss')\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "ax[1].plot(val_loss[1::])\n",
    "ax[1].set_title('Validation loss')\n",
    "plt.xlabel(\"Training iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
